# MNIST-Neural-Network
This project implements a 4-layer neural network from scratch using only NumPy, designed to classify handwritten digits from the MNIST dataset. The project focuses on understanding the inner workings of neural networks by manually coding forward propagation, backpropagation, and weight updates.

Features
Custom Neural Network Implementation: No external libraries like TensorFlow or PyTorchâ€”built entirely with NumPy.
4-Layer Architecture:
Input Layer: 784 neurons (28x28 pixels).
Hidden Layer 1: Configurable size (e.g., 128 neurons).
Hidden Layer 2: Configurable size (e.g., 64 neurons).
Output Layer: 10 neurons (one for each digit class).
Training and Testing:
Trains on the MNIST training set.
Evaluates accuracy on the MNIST test set.
Adjustable Hyperparameters: Easily modify learning rate, number of epochs, batch size, and layer sizes.
Educational Focus: Designed for learning how neural networks work under the hood.
